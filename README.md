<div align=center>

# üé® Awesome Diffusion Acceleration Cache üöÄ

<p>

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
![papercount](https://img.shields.io/badge/paper_count-100+-pink)

üëêüëê If you would like to contribute to this repository, feel free to email me at `ljc.mytcl@gmail.com`! üëêüëê

</p>

</div>

## üìö Repository Description
A curated list of research papers, resources, and advancements on Diffusion Cache and related efficient diffusion model acceleration techniques.

This repository aims to provide a comprehensive and up-to-date collection of academic works focused on Diffusion Cache ‚Äî a promising approach for accelerating diffusion models by caching intermediate features or latent states. It includes papers on model efficiency, memory optimization, reuse mechanisms, and inference speed-up in diffusion-based generative systems.

**This repository is maintained by EPIC Lab at Shanghai Jiao Tong University**


## üî• Update News

* **`2025/08/24`** ü§óü§ó We release an open-source repo for diffusion model acceleration and caching techniques!

## üìä Papers Table

|   # |    Date | Conference   | Paper Title                                                                                                                                                                                                                                                                                             | Institution                                   |
|-----|---------|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|
|   1 | 2023.05 | AAAI 2024    | **`[FISEdit ]`**<br>Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inferenc<br>[Paper](https://arxiv.org/pdf/2305.17423) [Code](https://github.com/Hankpipi/diffusers-hetu)                                                                                                      | Peking University                             |
|   2 | 2023.12 | CVPR 2024    | **`[DeepCache]`**<br>DeepCache: Accelerating Diffusion Models for Free<br>[Paper](https://arxiv.org/pdf/2312.00858) [Code](https://github.com/horseee/DeepCache)                                                                                                                                        | National University of Singapore              |
|   3 | 2023.12 | CVPR 2024    | **`[Block Caching]`**<br>Cache Me if You Can: Accelerating Diffusion Models through Block Caching<br>[Paper](https://arxiv.org/pdf/2312.03209)                                                                                                                                                          | Meta GenAI                                    |
|   4 | 2023.12 | NSDI'24      | **`[Approximate Caching]`**<br>Approximate Caching for Efficiently Serving Diffusion Models<br>[Paper](https://arxiv.org/pdf/2312.04429)                                                                                                                                                                | Adobe                                         |
|   5 | 2023.12 | NeurIPS 2024 | **`[FasterDiffusion]`**<br>Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference<br>[Paper](https://arxiv.org/pdf/2312.09608) [Code](https://sen-mao.github.io/FasterDiffusion)                                                                                            | Nankai University                             |
|   6 | 2024.04 | nan          | **`[T-GATE V1]`**<br>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models<br>[Paper](https://arxiv.org/pdf/2404.02747v1) [Code](https://github.com/HaozheLiu-ST/T-GATE)                                                                                                         | KAUST                                         |
|   7 | 2024.04 | TMLR         | **`[T-GATE V2]`**<br>Faster Diffusion via Temporal Attention Decompositionq<br>[Paper](https://arxiv.org/pdf/2404.02747) [Code](https://github.com/HaozheLiu-ST/T-GATE)                                                                                                                                 | KAUST                                         |
|   8 | 2024.06 | NeurIPS 2024 | **`[Layer Caching]`**<br>Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching<br>[Paper](https://arxiv.org/pdf/2406.01733) [Code](https://github.com/horseee/learning-to-cache/)                                                                                                     | National University of Singapore              |
|   9 | 2024.06 | nan          | **`[‚àÜ-DiT]`**<br>‚àÜ-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers<br>[Paper](https://arxiv.org/pdf/2406.01125)                                                                                                                                                            | Fudan                                         |
|  10 | 2024.06 | NeurIPS 2024 | **`[DiTFastAttn]`**<br>DiTFastAttn: Attention Compression for Diffusion Transformer Models<br>[Paper](https://arxiv.org/abs/2406.08552) [Code](https://github.com/thu-nics/DiTFastAttn)                                                                                                                 | Tsinghua                                      |
|  11 | 2024.07 | ECCV 2024    | **`[ElasticCache-LVLM]`**<br>Efficient Inference of Vision Instruction-Following Models with Elastic Cache<br>[Paper](https://arxiv.org/pdf/2407.18121) [Code](https://github.com/liuzuyan/ElasticCache)                                                                                                | Tsinghua                                      |
|  12 | 2024.07 | nan          | **`[FORA]`**<br>FORA: Fast-Forward Caching in Diffusion Transformer Acceleration<br>[Paper](https://arxiv.org/pdf/2407.01425) [Code](https://github.com/prathebaselva/FORA)                                                                                                                             | Microsoft                                     |
|  13 | 2024.07 | nan          | **`[VCUT]`**<br>Faster Image2Video Generation: A Closer Look at CLIP Image Embedding‚Äôs Impact on Spatio-Temporal Cross-Attentions<br>[Paper](https://arxiv.org/pdf/2407.19205)                                                                                                                          | The University of Western Australia           |
|  14 | 2024.09 | nan          | **`[TokenCache]`**<br>Token Caching for Diffusion Transformer Acceleration<br>[Paper](https://arxiv.org/pdf/2409.18523)                                                                                                                                                                                 | University of Chinese Academy of Sciences     |
|  15 | 2024.09 | ECCV 2024    | **`[FRDiff]`**<br>FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models<br>[Paper](https://arxiv.org/pdf/2312.03517) [Code](https://github.com/Jungwon-Lee/FRDiff)                                                                                                        | Pohang University of Science and Technology   |
|  16 | 2024.1  | nan          | **`[FasterCache]`**<br>FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality<br>[Paper](https://arxiv.org/pdf/2410.19355) [Code](https://github.com/Vchitect/FasterCache)                                                                                                     | nan                                           |
|  17 | 2024.1  | ICLR2025     | **`[ToCa]`**<br>ToCa: Accelerating Diffusion Transformers with Token-wise Feature Caching<br>[Paper](https://arxiv.org/abs/2410.05317) [Code](https://github.com/Shenyi-Z/ToCa)                                                                                                                         | Shanghai Jiao Tong University                 |
|  18 | 2024.11 | nan          | **`[AdaCache]`**<br>Adaptive Caching for Faster Video Generation with Diffusion Transformers<br>[Paper](https://adacache-dit.github.io/clarity/adacache_meta.pdf) [Code](https://github.com/AdaCache-DiT/AdaCache)                                                                                      | Meta                                          |
|  19 | 2024.11 | CVPR 2025    | **`[TeaCache]`**<br>Timestep Embedding Tells: It‚Äôs Time to Cache for Video Diffusion Model<br>[Paper](https://arxiv.org/pdf/2411.19108) [Code](https://github.com/LiewFeng/TeaCache)                                                                                                                    | Alibaba                                       |
|  20 | 2024.11 | AAAI 2025    | **`[LazyDiT]`**<br>LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers<br>[Paper](https://arxiv.org/pdf/2412.12444) [Code](https://github.com/shawnricecake/lazydit)                                                                                                                  | Adobe Research                                |
|  21 | 2024.11 | ICML 2025    | **`[Ca2-VDM]`**<br>Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing<br>[Paper](https://arxiv.org/pdf/2411.16375) [Code](https://github.com/Dawn-LX/CausalCache-VDM/)                                                                                    | Zhejiang University                           |
|  22 | 2024.11 | CVPR eLVM    | **`[SmoothCache]`**<br>SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers<br>[Paper](https://arxiv.org/pdf/2411.10510) [Code](https://github.com/Roblox/SmoothCache)                                                                                                  | Roblox                                        |
|  23 | 2024.12 | ICLR2025     | **`[DuCa]`**<br>Accelerating Diffusion Transformers with Dual Feature Caching<br>[Paper](https://arxiv.org/abs/2412.18911) [Code](https://github.com/Shenyi-Z/DuCa)                                                                                                                                     | Shanghai Jiao Tong University                 |
|  24 | 2025.01 | nan          | **`[FBCache]`**<br>Fastest HunyuanVideo Inference with Context Parallelism and First Block Cache on NVIDIA L20 GPUs<br>[Paper](-) [Code](ParaAttention/doc/fastest_hunyuan_video.md at main ¬∑ chengzeyi/ParaAttention --- ParaAttention/doc/fastest_hunyuan_video.md at main ¬∑ chengzeyi/ParaAttention) | nan                                           |
|  25 | 2025.01 | nan          | **`[FlexCache]`**<br>FlexCache: Flexible Approximate Cache System for Video Diffusion<br>[Paper](https://arxiv.org/abs/2501.04012)                                                                                                                                                                      | University of Waterloo                        |
|  26 | 2025.01 | nan          | **`[DaTo]`**<br>Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free<br>[Paper](https://arxiv.org/pdf/2501.00375)                                                                                                                                                        | Shanghai Jiao Tong University                 |
|  27 | 2025.02 | ICLR2025     | **`[PAB]`**<br>Real-Time Video Generation with Pyramid Attention Broadcast<br>[Paper](https://arxiv.org/pdf/2408.12588) [Code](https://github.com/NUS-HPC-AI-Lab/OpenDiT)                                                                                                                               | National University of Singapore              |
|  28 | 2025.03 | ICCV 2025    | **`[TaylorSeer]`**<br>From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers<br>[Paper](  https://arxiv.org/abs/2503.06923) [Code]( https://github.com/Shenyi-Z/TaylorSeer)                                                                                                                                                                                                                                                                                                         | Shanghai Jiao Tong University                 |
|  29 | 2025.03 | nan          | **`[FEB-Cache]`**<br>FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing Diffusion Transformer Caching<br>[Paper](https://arxiv.org/pdf/2503.07120) [Code](https://github.com/aSleepyTree/EB-Cache)                                                                                       | University of Science and Technology of China |
|  30 | 2025.03 | CVPR 2025    | **`[CacheQuant]`**<br>CacheQuant: Comprehensively Accelerated Diffusion Models<br>[Paper](https://arxiv.org/pdf/2503.01323) [Code](https://github.com/BienLuky/CacheQuant)                                                                                                                              | University of Chinese Academy of Sciences     |
|  31 | 2025.03 | ICCV 2025    | **`[QuantCache]`**<br>QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation<br>[Paper](https://arxiv.org/pdf/2503.06545) [Code](https://github.com/JunyiWuCode/QuantCache)                                                                | Shanghai Jiao Tong University                 |
|  32 | 2025.04 | nan          | **`[AB-Cache]`**<br>AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse<br>[Paper](https://arxiv.org/abs/2504.10540)                                                                                                                                      | University of Science and Technology of China |
|  33 | 2025.04 | ACM MM 2025  | **`[ClusCa]`**<br>Compute only 16 tokens in one timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching<br>[Paper](-) [Code]( https://github.com/zhixin-zheng/ClusCa)                                                                                                         | Shanghai Jiao Tong University                 |
|  34 | 2025.04 | ACM MM 2025  | **`[SpeCa]`**<br>SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching<br>[Paper](-) [Code](https://github.com/Shenyi-Z/Cache4Diffusion/)                                                                                                                                         | Shanghai Jiao Tong University                 |
|  35 | 2025.04 | nan          | **`[FeMO]`**<br>Accelerate Diffusion Transformers with Feature Momentum<br>[Paper](-) [Code](https://github.com/Shenyi-Z/Cache4Diffusion/)                                                                                                                                                              | Shanghai Jiao Tong University                 |
|  36 | 2025.04 | CVPR 2025    | **`[ICC]`**<br>Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition<br>[Paper](https://arxiv.org/pdf/2505.05829) [Code](https://github.com/ccccczzy/icc)                                                                                 | Peking University                             |
|  37 | 2025.05 | CVPR 2025    | **`[FastCache]`**<br>FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation<br>[Paper](https://arxiv.org/abs/2505.20353) [Code](https://github.com/NoakLiu/FastCache-xDiT)                                                                                            | Yale University                               |
|  38 | 2025.06 | nan          | **`[DBPrune]`**<br>DBPrune: Dynamic Block Prune with Residual Caching<br>[Paper](-) [Code](https://github.com/vipshop/cache-dit)                                                                                                                                                                                                                  | Vipshop                                       |
|  39 | 2025.06 | nan          | **`[DBCache]`**<br>DBCache: Dual Block Caching for Diffusion Transformers<br>[Paper](-)  [Code](https://github.com/vipshop/cache-dit)                                                                                                                                                                                                                    | Vipshop                                       |
|  40 | 2025.06 | nan          | **`[BACache]`**<br>Block-wise Adaptive Caching for Accelerating Diffusion Policy<br>[Paper](https://arxiv.org/pdf/2506.13456)                                                                                                                                                                           | Peking University                             |
|  41 | 2025.07 | ICCV 2025    | **`[SkipCache]`**<br>Accelerating Vision Diffusion Transformers with Skip Branches<br>[Paper](https://arxiv.org/abs/2411.17616) [Code](https://github.com/OpenSparseLLMs/Skip-DiT)                                                                                                                      | Shanghai Jiao Tong University                 |
|  42 | 2025.07 | nan          | **`[FoCa ]`**<br>Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers<br>[Paper](https://arxiv.org/abs/2508.16211) [Code](https://github.com/Shenyi-Z/Cache4Diffusion/)                                                                                                 | Shanghai Jiao Tong University                 |
|  43 | 2025.07 | nan          | **`[HiCache]`**<br>HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching<br>[Paper](https://arxiv.org/abs/2508.16984) [Code](https://github.com/Shenyi-Z/Cache4Diffusion/)                                                                               | Shanghai Jiao Tong University                 |
|  44 | 2025.07 | nan          | **`[WaveEx]`**<br>WaveEx: Accelerating Flow Matching-based Speech Generation via Wavelet-guided Extrapolation<br>[Paper](-) [Code](https://github.com/Shenyi-Z/Cache4Diffusion/)                                                                                                                        | Shanghai Jiao Tong University                 |
|  45 | 2025.07 | ICCV 2025    | **`[GOC]`**<br>Accelerating Diffusion Transformer via Gradient-Optimized Cache<br>[Paper](https://arxiv.org/pdf/2503.05156) [Code](https://github.com/qiujx0520/GOC_ICCV2025)                                                                                                                           | University of Science and Technology of China |
|  46 | 2025.08 | nan          | **`[TaoCache]`**<br>TaoCache: Structure-Maintained Video Generation Acceleration<br>[Paper](https://arxiv.org/pdf/2508.08978)                                                                                                                                                                           | Huawei                                        |
|  47 | 2025.08 | ICCV 2025    | **`[OmniCache]`**<br>OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models<br>[Paper](https://arxiv.org/abs/2508.16212)                                                                                                                     | Zhipu AI                                      |
|  48 | 2025.08 | nan          | **`[MixCache]`**<br>MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration<br>[Paper](https://arxiv.org/abs/2508.12691v1)                                                                                                                                                              | Sun Yat-sen University                        |
|  49 | 2025.09 | nan          | **`[Z-Cache]`**<br>Z-Cache: Accelerating Diffusion Transformers via  Self-Reflection<br>[Paper](-) [Code](https://github.com/Shenyi-Z/Cache4Diffusion/)                                                                                                                                                 | Shanghai Jiao Tong University                 |






## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

Thanks to all researchers and contributors who have worked on diffusion model acceleration and caching techniques. 
