# Awesome-Diffusion-Acceleration-Cache
## é¡¹ç›®æè¿°
è¿™æ˜¯ä¸€ä¸ªå…³äºæ‰©æ•£æ¨¡å‹åŠ é€Ÿç¼“å­˜æŠ€æœ¯çš„ç ”ç©¶è®ºæ–‡ã€èµ„æºå’Œè¿›å±•çš„ç²¾é€‰åˆ—è¡¨ã€‚

æœ¬ä»“åº“æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢ä¸”æœ€æ–°çš„å­¦æœ¯å·¥ä½œé›†åˆï¼Œä¸“æ³¨äºæ‰©æ•£ç¼“å­˜â€”â€”ä¸€ç§é€šè¿‡ç¼“å­˜ä¸­é—´ç‰¹å¾æˆ–æ½œåœ¨çŠ¶æ€æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚å®ƒåŒ…æ‹¬å…³äºæ¨¡å‹æ•ˆç‡ã€å†…å­˜ä¼˜åŒ–ã€é‡ç”¨æœºåˆ¶å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆç³»ç»Ÿä¸­æ¨ç†åŠ é€Ÿçš„è®ºæ–‡ã€‚

## ğŸ“Š è®ºæ–‡è¡¨æ ¼

| å‘è¡¨æ—¶é—´ | ä¼šè®®/æœŸåˆŠ | æ–¹æ³•åç§° | è®ºæ–‡åç§° | æ¶æ„ | æ–¹å‘ | è®ºæ–‡å•ä½ | è®ºæ–‡é“¾æ¥ | ä»£ç ä»“åº“ |
|---------|-----------|----------|----------|------|------|----------|----------|----------|
| 2023.05 | AAAI 2024 | FISEdit | Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inference | Unet | å›¾ç‰‡ç”Ÿæˆ | Peking University | [è®ºæ–‡](https://arxiv.org/pdf/2305.17423) | [ä»£ç ](https://github.com/Hankpipi/diffusers-hetu) |
| 2023.12 | - | DeepCache | DeepCache: Accelerating Diffusion Models for Free | Unet | - | National University of Singapore | [è®ºæ–‡](https://arxiv.org/pdf/2312.00858) | [ä»£ç ](https://github.com/horseee/DeepCache) |
| 2023.12 | - | Block Caching | Cache Me if You Can: Accelerating Diffusion Models through Block Caching | Unet | - | Meta GenAI | [è®ºæ–‡](https://arxiv.org/pdf/2312.03209) | - |
| 2023.12 | - | Approximate Caching | Approximate Caching for Efficiently Serving Diffusion Models | - | - | Adobe | [è®ºæ–‡](https://arxiv.org/pdf/2312.04429) | - |
| 2024.04 | - | T-GATE V1 | Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models | - | - | KAUST | [è®ºæ–‡](https://arxiv.org/pdf/2404.02747v1) | [ä»£ç ](https://github.com/HaozheLiu-ST/T-GATE) |
| 2024.04 | - | T-GATE V2 | Faster Diffusion via Temporal Attention Decomposition | - | - | KAUST | [è®ºæ–‡](https://arxiv.org/pdf/2404.02747) | - |
| 2024.06 | - | Layer Caching | Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching | - | - | National University of Singapore | [è®ºæ–‡](https://arxiv.org/pdf/2406.01733) | [ä»£ç ](https://github.com/horseee/learning-to-cache/) |
| 2024.06 | - | âˆ†-DiT | âˆ†-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers | DiT | å›¾ç‰‡ç”Ÿæˆ | Fudan | [è®ºæ–‡](https://arxiv.org/pdf/2406.01125) | - |
| 2024.07 | - | ElasticCache-LVLM | Efficient Inference of Vision Instruction-Following Models with Elastic Cache | - | - | Tsinghua | [è®ºæ–‡](https://arxiv.org/pdf/2407.18121) | [ä»£ç ](https://github.com/liuzuyan/ElasticCache) |
| 2024.07 | - | FORA | FORA: Fast-Forward Caching in Diffusion Transformer Acceleration | DiT | å›¾ç‰‡ç”Ÿæˆ | Microsoft | [è®ºæ–‡](https://arxiv.org/pdf/2407.01425) | [ä»£ç ](https://github.com/prathebaselva/FORA) |
| 2024.07 | - | VCUT | Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions | - | - | The University of Western Australia | [è®ºæ–‡](https://arxiv.org/pdf/2407.19205) | - |
| 2024.09 | - | TokenCache | Token Caching for Diffusion Transformer Acceleration | - | - | University of Chinese Academy of Sciences | [è®ºæ–‡](https://arxiv.org/pdf/2409.18523) | - |
| 2024.09 | - | FRDiff | FRDiff: Feature Reuse for Universal Training-free Acceleration of Diffusion Models | - | - | Pohang University of Science and Technology | [è®ºæ–‡](https://arxiv.org/pdf/2312.03517) | [ä»£ç ](https://github.com/Jungwon-Lee/FRDiff) |
| 2024.10 | - | FasterCache | FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality | - | - | - | [è®ºæ–‡](https://arxiv.org/pdf/2410.19355) | [ä»£ç ](https://github.com/Vchitect/FasterCache) |
| 2024.10 | ICLR 2025 | ToCa | ToCa: Accelerating Diffusion Transformers with Token-wise Feature Caching | DiT | - | Shanghai Jiao Tong University | - | - |
| 2024.11 | - | AdaCache | Adaptive Caching for Faster Video Generation with Diffusion Transformers | - | è§†é¢‘ç”Ÿæˆ | Meta | [è®ºæ–‡](https://adacache-dit.github.io/clarity/adacache_meta.pdf) | [ä»£ç ](https://github.com/AdaCache-DiT/AdaCache) |
| 2024.11 | CVPR 2025 | TeaCache | Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model | - | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Alibaba | [è®ºæ–‡](https://arxiv.org/pdf/2411.19108) | [ä»£ç ](https://github.com/LiewFeng/TeaCache) |
| 2024.11 | - | LazyDiT | LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers | DiT | è§†é¢‘ç”Ÿæˆ | Adobe Research | [è®ºæ–‡](https://arxiv.org/pdf/2412.12444) | - |
| 2024.11 | - | Ca2-VDM | Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing | - | - | - | [è®ºæ–‡](https://arxiv.org/pdf/2411.16375) | [ä»£ç ](https://github.com/Dawn-LX/CausalCache-VDM/) |
| 2024.11 | - | SmoothCache | SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers | - | - | Roblox | [è®ºæ–‡](https://arxiv.org/pdf/2411.10510) | [ä»£ç ](https://github.com/Roblox/SmoothCache) |
| 2024.11 | - | SkipCache | Accelerating Vision Diffusion Transformers with Skip Branches | - | - | Shanghai Jiao Tong University | - | - |
| 2024.12 | ICLR 2025 | DuCa | Accelerating Diffusion Transformers with Dual Feature Caching | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | - |
| 2025.01 | - | FBCache | Fastest HunyuanVideo Inference with Context Parallelism and First Block Cache on NVIDIA L20 GPUs | - | - | - | - | - |
| 2025.01 | - | FlexCache | FlexCache: Flexible Approximate Cache System for Video Diffusion | - | - | - | - | - |
| 2025.01 | - | DaTo | Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free | - | - | Shanghai Jiao Tong University | [è®ºæ–‡](https://arxiv.org/pdf/2501.00375) | - |
| 2025.02 | - | PAB | Real-Time Video Generation with Pyramid Attention Broadcast | - | - | National University of Singapore | [è®ºæ–‡](https://arxiv.org/pdf/2408.12588) | [ä»£ç ](https://github.com/NUS-HPC-AI-Lab/OpenDiT) |
| 2025.03 | ICCV 2025 | TaylorSeer | From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/TaylorSeer) |
| 2025.03 | - | FEB-Cache | FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing Diffusion Transformer Caching | - | - | University of Science and Technology of China | [è®ºæ–‡](https://arxiv.org/pdf/2503.07120) | [ä»£ç ](https://github.com/aSleepyTree/EB-Cache) |
| 2025.03 | - | CacheQuant | CacheQuant: Comprehensively Accelerated Diffusion Models | - | - | University of Chinese Academy of Sciences | [è®ºæ–‡](https://arxiv.org/pdf/2503.01323) | [ä»£ç ](https://github.com/BienLuky/CacheQuant) |
| 2025.03 | - | QuantCache | QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation | - | - | Shanghai Jiao Tong University | [è®ºæ–‡](https://arxiv.org/pdf/2503.06545) | [ä»£ç ](https://github.com/JunyiWuCode/QuantCache) |
| 2025.04 | - | AB-Cache | AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse | - | - | - | - | - |
| 2025.04 | ACM MM 2025 | ClusCa | Compute only 16 tokens in one timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/zhixin-zheng/ClusCa) |
| 2025.04 | ACM MM 2025 | SpeCa | SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/Cache4Diffusion/) |
| 2025.04 | - | FeMO | Accelerate Diffusion Transformers with Feature Momentum | DiT | å›¾ç‰‡ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/Cache4Diffusion/) |
| 2025.04 | - | ICC | Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition | - | - | Peking University | [è®ºæ–‡](https://arxiv.org/pdf/2505.05829) | [ä»£ç ](https://github.com/ccccczzy/icc) |
| 2025.05 | - | FastCache | FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation | - | - | - | - | - |
| 2025.06 | - | DBPrune | DBPrune: Dynamic Block Prune with Residual Caching | - | - | Vipshop | - | - |
| 2025.06 | - | DBCache | DBCache: Dual Block Caching for Diffusion Transformers | - | - | Vipshop | - | - |
| 2025.06 | - | BACache | Block-wise Adaptive Caching for Accelerating Diffusion Policy | - | - | Peking University | [è®ºæ–‡](https://arxiv.org/pdf/2506.13456) | - |
| 2025.07 | - | FoCa | Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/Cache4Diffusion/) |
| 2025.07 | - | HiCache | HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/Cache4Diffusion/) |
| 2025.07 | - | WaveEx | WaveEx: Accelerating Flow Matching-based Speech Generation via Wavelet-guided Extrapolation | - | è¯­éŸ³ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/Cache4Diffusion/) |
| 2025.07 | - | GOC | Accelerating Diffusion Transformer via Gradient-Optimized Cache | - | - | University of Science and Technology of China | [è®ºæ–‡](https://arxiv.org/pdf/2503.05156) | [ä»£ç ](https://github.com/qiujx0520/GOC_ICCV2025) |
| 2025.08 | - | TaoCache | TaoCache: Structure-Maintained Video Generation Acceleration | - | - | Huawei | [è®ºæ–‡](https://arxiv.org/pdf/2508.08978) | - |
| 2025.09 | - | Z-Cache | Z-Cache: Accelerating Diffusion Transformers via Self-Reflection | DiT | å›¾ç‰‡ç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ | Shanghai Jiao Tong University | - | [ä»£ç ](https://github.com/Shenyi-Z/Cache4Diffusion/) |

## ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯

- **æ€»è®ºæ–‡æ•°**: 47
- **æœ¬ç»„è®ºæ–‡**: 12
- **æœ‰ä»£ç çš„è®ºæ–‡**: 25
- **é¡¶çº§ä¼šè®®è®ºæ–‡**: 8

## ğŸ¯ ç ”ç©¶æ–¹å‘

### æ¶æ„ç±»å‹
- **Unet-based**: 3ç¯‡è®ºæ–‡
- **DiT-based**: 15ç¯‡è®ºæ–‡
- **å…¶ä»–**: 29ç¯‡è®ºæ–‡

### åº”ç”¨é¢†åŸŸ
- **å›¾ç‰‡ç”Ÿæˆ**: 15ç¯‡è®ºæ–‡
- **è§†é¢‘ç”Ÿæˆ**: 12ç¯‡è®ºæ–‡
- **è¯­éŸ³ç”Ÿæˆ**: 1ç¯‡è®ºæ–‡
- **é€šç”¨**: 19ç¯‡è®ºæ–‡

## ğŸ”— æœ‰ç”¨é“¾æ¥

- [Paperlists.csv](./Paperlists.csv) - åŒ…å«è¯¦ç»†ä¿¡æ¯çš„å®Œæ•´è®ºæ–‡åˆ—è¡¨

## ğŸ“ å¦‚ä½•è´¡çŒ®

1. Fork æœ¬ä»“åº“
2. åœ¨ `Paperlists.csv` ä¸­æ·»åŠ æ–°è®ºæ–‡
3. æ›´æ–° README è¡¨æ ¼
4. æäº¤ pull request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·å‚è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰è‡´åŠ›äºæ‰©æ•£æ¨¡å‹åŠ é€Ÿå’Œç¼“å­˜æŠ€æœ¯ç ”ç©¶çš„ç ”ç©¶è€…å’Œè´¡çŒ®è€…ã€‚ 