发表时间,会议（没中或者是报告的话填None）,方法名称,论文名字,架构,方向,论文单位（全称）,论文链接,论文仓库,论文创新点（CH）100字以内,论文创新点（EN）,本组工作,父记录
2023.05,AAAI 2024,FISEdit ,Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inferenc,Unet,图片生成,Peking University,https://arxiv.org/pdf/2305.17423,https://github.com/Hankpipi/diffusers-hetu," 本文提出 FISEdit 框架，通过识别语义变化影响区域并复用未修改区域的特征缓存，实现稀疏更新。实测在 TITAN RTX 和 A100 GPU 上推理速度分别提升约 4.4× 和 3.4×，且图像质量得到强化。",,,
2023.12,CVPR 2024,DeepCache,DeepCache: Accelerating Diffusion Models for Free,Unet,图片生成,National University of Singapore,https://arxiv.org/pdf/2312.00858,https://github.com/horseee/DeepCache,该方法利用U-Net架构中高级特征在相邻去噪步骤间的时间冗余性。通过缓存并复用上采样（Decoder）部分的高级特征，它跳过深层网络的冗余计算，同时允许低级特征通过浅层路径更新。其提出的非均匀缓存策略能根据特征变化动态调整计算，实现高效的免训练加速。,,,
2023.12,CVPR 2024,Block Caching,Cache Me if You Can: Accelerating Diffusion Models through Block Caching,Unet,图片生成,Meta GenAI,https://arxiv.org/pdf/2312.03209,None,该方法深入分析U-Net内部各区块（Block）在去噪过程中的动态变化，发现不同区块的特征变化模式各异且平滑。其核心创新是提出了“区块缓存”策略，并能根据特征变化率自动生成最优缓存调度方案，对变化小的区块进行选择性缓存。此外，它引入了轻量化的“尺度-位移”微调机制，以校正缓存带来的特征偏移，提升生成质量。,,,
2023.12,NSDI'24,Approximate Caching,Approximate Caching for Efficiently Serving Diffusion Models,,图片生成,Adobe,https://arxiv.org/pdf/2312.04429,,该方法专为 扩散模型服务加速 设计，基于缓存机制优化生成过程。首先，它发现 中间噪声状态存在跨步复用性，提出近似缓存框架以跳过重复计算。其次，它设计了 LCBFU 缓存策略，根据计算收益与频率动态选择缓存内容。结合这两点，系统 Nirvana 在保持生成质量的同时，大幅降低了延迟与 GPU 成本，实现了高效推理加速。,,,
2023.12,NeurIPS 2024,FasterDiffusion,Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference,Unet,图片生成,Nankai University,https://arxiv.org/pdf/2312.09608,https://sen-mao.github.io/FasterDiffusion,该方法的核心发现是U-Net的编码器（Encoder）特征在去噪步骤间变化微小，而解码器（Decoder）特征变化显著。据此，它提出“编码器传播”策略：缓存并复用单次计算的编码器特征，以并行处理多个后续的解码器步骤，极大提升了推理速度。同时，引入先验噪声注入技术以补偿细节纹理，是一种高效的免训练并行加速方案。,,,
2024.04,None,T-GATE V1,Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models,,图片生成,KAUST,https://arxiv.org/pdf/2404.02747v1,https://github.com/HaozheLiu-ST/T-GATE,,,,
2024.04,TMLR,T-GATE V2,Faster Diffusion via Temporal Attention Decompositionq,"Unet, DiT",图片生成,KAUST,https://arxiv.org/pdf/2404.02747,https://github.com/HaozheLiu-ST/T-GATE,该方法揭示 attention 的前后阶段作用差异，提出简单高效训练 Tgate 方法，通过按阶段缓存与跳过 attention 模块，实现 10%–50% 推理加速，近乎无损质量,,,
2024.06,NeurIPS 2024,Layer Caching,Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching,DiT,图片生成,National University of Singapore,https://arxiv.org/pdf/2406.01733,https://github.com/horseee/learning-to-cache/,该方法利用 timestep 间层级冗余，提出 L2C 框架，通过可微分优化方法训练一个 输入不变但时间步可变的路由器（router），决定哪些层可以跳过计算、哪些要执行，从而生成静态计算图。无需额外训练，也避免了指数级搜索空间问题。缓存大比例层级计算（如 U-ViT-H/2 缓存 93.68%），几乎无损 FID，显著降低计算量,,,
2024.06,None,∆-DiT,∆-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers,DiT,图片生成,Fudan,https://arxiv.org/pdf/2406.01125,,该方法专为DiT架构设计，其核心创新点有二。首先，它发现DiT的前序模块主要生成轮廓，后序模块则负责细节。其次，为解决DiT直接缓存导致的信息丢失问题，它提出了∆-Cache，即缓存特征的“增量”（残差）而非绝对值。结合这两点，∆-DiT在生成早期（轮廓阶段）缓存后序模块，在后期（细节阶段）缓存前序模块，实现了与生成过程相匹配的自适应加速。,,,
2024.06,NeurIPS 2024,DiTFastAttn,DiTFastAttn: Attention Compression for Diffusion Transformer Models,DiT,图片生成,Tsinghua,https://arxiv.org/abs/2406.08552,https://github.com/thu-nics/DiTFastAttn,该方法系统性地识别并解决DiT中的三种注意力冗余：空间、时间与条件（CFG） 。其核心创新是一个技术组合：1）“带残差共享的窗口注意力”（WA-RS）在降低空间计算的同时保留全局信息 ；2）跨时间步（AST）与跨CFG（ASC）的注意力共享，以复用相似计算 。最后通过贪心算法为每层每步智能选择最优压缩策略，实现高效的免训练加速 。,,,
2024.07,ECCV 2024,ElasticCache-LVLM,Efficient Inference of Vision Instruction-Following Models with Elastic Cache,,,Tsinghua,https://arxiv.org/pdf/2407.18121,https://github.com/liuzuyan/ElasticCache,Elastic Cache：编码阶段重要性驱动 KV 合并；生成阶段借助 fixed-point 策略；创新采用“Two Policies”分段缓存管理方式，有效压缩缓存冗余，同时保留生成质量,,,
2024.07,None,FORA,FORA: Fast-Forward Caching in Diffusion Transformer Acceleration,DiT,图片生成,Microsoft,https://arxiv.org/pdf/2407.01425,https://github.com/prathebaselva/FORA,该方法是一种针对DiT架构的简洁高效的免训练缓存方案。其核心创新在于，它将缓存目标从整个DiT区块细化到其内部最耗费计算的两个模块：自注意力和MLP层 。FORA采用固定的静态缓存间隔（N），即每N步完整计算一次并缓存特征，在随后的N-1步中直接复用，通过跳过区块内部的核心计算来实现加速 。,,,
2024.07,None,VCUT,Faster Image2Video Generation: A Closer Look at CLIP Image Embedding’s Impact on Spatio-Temporal Cross-Attentions,,视频生成,The University of Western Australia,https://arxiv.org/pdf/2407.19205,,分析发现 CLIP 和 Cross-Attention 存在冗余，引入 VCUT：一种训练自由（training-free）的优化策略。它在生成第一步中使用一个线性层替代计算密集的空间 Cross-Attention 组件，然后缓存该线性输出，并在后续所有推理步骤中复用，从而极大地降低计算需求。该方法无需持续更新注意力机制就能维持高质量生成，同时每个视频减少约 322T MACs，参数削减约 50M，延迟缩减约 20%。,,,
2024.09,None,TokenCache,Token Caching for Diffusion Transformer Acceleration,DiT,图片生成,University of Chinese Academy of Sciences,https://arxiv.org/pdf/2409.18523,,该方法首次将缓存策略从区块级细化到令牌（Token）级 。其核心创新是引入一个可学习的“缓存预测器”，为每个令牌分配重要性分数，从而实现对冗余令牌的选择性剪枝与复用 。它结合了自适应区块选择策略（仅在最不重要的区块剪枝）和两阶段循环调度（TPRR）策略，在不同生成阶段动态调整缓存间隔，实现高效的DiT加速 。,,,
2024.09,ECCV 2024,FRDiff,FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models,Unet,图片生成,Pohang University of Science and Technology,https://arxiv.org/pdf/2312.03517,https://github.com/Jungwon-Lee/FRDiff,"通过复用这些扩散推理中相邻时间步高度相似特征图跳过冗余计算以节省资源且不会降低图像质量，采用“特征复用（Feature Reuse, FR）”合并“减少 denoising 步数（reduced NFE）”以释放性能；利用 auto-tuning 模块（Auto-FR）根据延迟或质量需求自动调节复用策略；再通过 Score Mixing 平滑生成质量与计算效率之间的权衡。",,,
2024.1,None,FasterCache,FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality,,视频生成,,https://arxiv.org/pdf/2410.19355,https://github.com/Vchitect/FasterCache,本文提出一种 FasterCache 策略，包括动态特征重用保持时间连续性与区分性，以及 CFG-Cache 优化条件与无条件特征的复用，实现训练免加速，在 Vchitect-2.0 上达约 1.67× 提速且视频质量不降。,,,
2024.1,ICLR2025,ToCa,ToCa: Accelerating Diffusion Transformers with Token-wise Feature Caching,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,https://arxiv.org/abs/2410.05317,https://github.com/Shenyi-Z/ToCa,介绍了一种名为 ToCa 的 Token 级特征缓存方法，这是一种用于扩散变压器的细粒度缓存方法。该方法根据时间冗余和误差传播敏感性自适应地选择 Token，并在不同层应用不同的缓存比例。在 OpenSora 上实现了约 2.36 倍的无需训练的加速效果，同时保持了近乎无损的生成质量。,,是,
2024.11,None,AdaCache,Adaptive Caching for Faster Video Generation with Diffusion Transformers,DiT,视频生成,Meta,https://adacache-dit.github.io/clarity/adacache_meta.pdf,https://github.com/AdaCache-DiT/AdaCache,这项工作提出了 AdaCache，一种针对视频 DiTs 的训练无关自适应缓存方法，它通过运动正则化（MoReg）方案根据视频内容调整缓存计划，以控制计算量。该方法在 Open-Sora 720p 视频上实现了高达 4.7 倍的推理加速，且不损失质量。,,,
2024.11,CVPR 2025,TeaCache,Timestep Embedding Tells: It’s Time to Cache for Video Diffusion Model,,"图片生成, 视频生成",Alibaba,https://arxiv.org/pdf/2411.19108,https://github.com/LiewFeng/TeaCache,介绍了一种无需训练的缓存方法（TeaCache），通过时间步长嵌入来估计输出方差，选择性地在计算冗余的时间步长进行缓存，实现高达~4.41 倍的加速，同时视觉质量退化可忽略不计。,,,
2024.11,AAAI 2025,LazyDiT,LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers,DiT,视频生成,Adobe Research,https://arxiv.org/pdf/2412.12444,https://github.com/shawnricecake/lazydit,这项工作介绍了 LazyDiT，一个将可训练的惰性学习层嵌入到 DiTs 中的惰性学习框架，通过重用先前的步骤结果来动态跳过冗余计算。它在图像和视频模型上实现了推理加速，且质量损失可以忽略不计。,,,
2024.11,ICML 2025,Ca2-VDM,Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing,,视频生成,Zhejiang University,https://arxiv.org/pdf/2411.16375,https://github.com/Dawn-LX/CausalCache-VDM/,本文提出 Ca2-VDM，通过因果生成避免邻近帧重复计算，并引入跨步骤缓存共享机制，显著降低计算冗余与存储成本，实现高效长段视频生成加速。,,,
2024.11,CVPR eLVM,SmoothCache,SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers,DiT,"图片生成, 视频生成, 音频生成",Roblox,https://arxiv.org/pdf/2411.10510,https://github.com/Roblox/SmoothCache,提出 SmoothCache：一种无需训练的通用缓存机制，通过分析相邻 timestep 层输出误差自适应缓存关键特征，在 DiT 架构中实现图像、视频、音频等多种模态推理加速 8%–71%，质量保持或略有提升。,,,
2024.12,ICLR2025,DuCa,Accelerating Diffusion Transformers with Dual Feature Caching,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,https://arxiv.org/abs/2412.18911,https://github.com/Shenyi-Z/DuCa,DuCa 提出了一种双重特征缓存策略：迭代地在激进缓存（用于高速提升）和保守缓存（用于纠正累积错误）之间交替，并提出了 V-Caching，这是一种基于值范数重要性的逐标记保守缓存方法，与 FlashAttention 兼容。这种无需训练的方法在保持生成质量的同时，实现了显著的推理加速。,,是,
2025.01,None,FBCache,Fastest HunyuanVideo Inference with Context Parallelism and First Block Cache on NVIDIA L20 GPUs,,视频生成,,,ParaAttention/doc/fastest_hunyuan_video.md at main · chengzeyi/ParaAttention --- ParaAttention/doc/fastest_hunyuan_video.md at main · chengzeyi/ParaAttention,实现上下文并行和第一块缓存，以并行推理并缓存残差差异较小时早期 Transformer 块的输出，在多 GPU 缩放下，在 NVIDIA L20 GPU 上实现高达 ~5.66 倍的推理加速。还集成了 FP8 动态量化和 torch.compile 以进一步优化。,,,
2025.01,None,FlexCache,FlexCache: Flexible Approximate Cache System for Video Diffusion,,视频生成,University of Waterloo,https://arxiv.org/abs/2501.04012,,FlexCache 提出了一种使用压缩（存储空间减少 6.7 倍）和对象-背景解耦的灵活近似缓存系统，以提高命中率与效率，在视频扩散模型中实现了吞吐量提升 1.26 倍和成本降低 25%。,,,
2025.01,None,DaTo,Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free,Unet,图片生成,Shanghai Jiao Tong University,https://arxiv.org/pdf/2501.00375,,提出 Dynamics-aware token pruning (DaTo)，结合特征缓存与令牌剪枝，仅保留高动态性令牌以扩展时间步特征动态性，训练免加速 Stable Diffusion，ImageNet 上 9× 提速同时 FID 降 0.33，COCO-30k 上 7× 提速且 FID 大幅下降（–2.17）。,,是,
2025.02,ICLR2025,PAB,Real-Time Video Generation with Pyramid Attention Broadcast,DiT,视频生成,National University of Singapore,https://arxiv.org/pdf/2408.12588,https://github.com/NUS-HPC-AI-Lab/OpenDiT,提出 Pyramid Attention Broadcast，通过分层广播 attention 输出 & 并行推理，实现在 DiT 上高质量 720p 视频的实时生成（最高 10.5× 加速）。,,,
2025.03,ICCV 2025,TaylorSeer,"From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers
",DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,"  https://arxiv.org/abs/2503.06923"," https://github.com/Shenyi-Z/TaylorSeer","  提出了一种使用泰勒级数展开的缓存后预测范式，用于预测扩散模型中未来时间步长的特征，在不进行额外训练的情况下实现了近乎无损的加速（图像速度提升约 4.99 倍，视频提升约 5.00 倍）。",,是,
2025.03,None,FEB-Cache,FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing Diffusion Transformer Caching,DiT,图片生成,University of Science and Technology of China,https://arxiv.org/pdf/2503.07120,https://github.com/aSleepyTree/EB-Cache,FEB-Cache 引入了一种频率引导、分离的缓存策略用于 Attention 和 MLP，并采用阶段特定的噪声缩放来缓解缓存放大曝光偏差的问题，从而实现图像质量的提升和推理加速的效率。,,,
2025.03,CVPR 2025,CacheQuant,CacheQuant: Comprehensively Accelerated Diffusion Models,Unet,图片生成,University of Chinese Academy of Sciences,https://arxiv.org/pdf/2503.01323,https://github.com/BienLuky/CacheQuant,"提出了一种无需训练的 CacheQuant 框架，通过动态规划和解耦误差校正联合优化缓存和量化，在仅损失 0.02 CLIP 分数的情况下，为 Stable Diffusion 实现了 5.18 倍加速和 4 倍压缩。同时优化缓存调度与模型量化，而非分级进行，两者结合效果更佳；使用 动态规划（Dynamic Programming Schedule, DPS） 计算最优缓存与量化方案，以最小化误差；引入 解耦误差修正（Decoupled Error Correction, DEC），逐步修正缓存和量化带来的累积误差；在 MS-COCO 的 Stable Diffusion 上实现 5.18× 推理加速和4× 模型压缩，CLIP 分数仅下降 0.02。",,,
2025.03,ICCV 2025,QuantCache,QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation,DiT,视频生成,Shanghai Jiao Tong University,https://arxiv.org/pdf/2503.06545,https://github.com/JunyiWuCode/QuantCache,提出 QuantCache，一种训练免的加速框架，通过层级缓存、重要性引导量化与结构冗余剪枝三位一体优化，实现 Open-Sora 上约 6.72× 延迟加速且质量损失极小。,,,
2025.04,None,AB-Cache,AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse,"Unet, DiT","图片生成, 视频生成",University of Science and Technology of China,https://arxiv.org/abs/2504.10540,,AB-Cache 利用了一种 Adams-Bashforth 缓存机制：它不是直接重用，而是应用缓存的特征的更高阶线性组合，具有可证明的截断误差为 O(hᵏ)，在保持生成质量的同时，实现了近 3 倍的推理加速。,,,
2025.04,ACM MM 2025,ClusCa,Compute only 16 tokens in one timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,None," https://github.com/zhixin-zheng/ClusCa",,,是,
2025.04,ACM MM 2025,SpeCa,SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,None,https://github.com/Shenyi-Z/Cache4Diffusion/,,,是,
2025.04,None,FeMO,Accelerate Diffusion Transformers with Feature Momentum,DiT,图片生成,Shanghai Jiao Tong University,None,https://github.com/Shenyi-Z/Cache4Diffusion/,,,是,
2025.04,CVPR 2025,ICC,Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition,DiT,图片生成,Peking University,https://arxiv.org/pdf/2505.05829,https://github.com/ccccczzy/icc,提出 Increment-Calibrated Caching：基于低秩推广方法自动生成校准参数，通过 channel-aware SVD 修正异常通道，提升缓存准确度与加速效果，无训练方式提升推理效率。,,,
2025.05,CVPR 2025,FastCache,FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation,DiT,图片生成,Yale University,https://arxiv.org/abs/2505.20353,https://github.com/NoakLiu/FastCache-xDiT,FastCache 结合了空间感知的 token 选择和 transformer 级别的缓存，通过可学习的线性近似来跳过冗余计算，在保持生成保真度的同时，实现了显著的延迟和内存减少。,,,
2025.06,,DBPrune,DBPrune: Dynamic Block Prune with Residual Caching,,,Vipshop,,,,,,
2025.06,,DBCache,DBCache: Dual Block Caching for Diffusion Transformers,,,Vipshop,,,,,,
2025.06,None,BACache,Block-wise Adaptive Caching for Accelerating Diffusion Policy,,,Peking University,https://arxiv.org/pdf/2506.13456,,提出了 Block-wise Adaptive Caching（BAC）方法，通过在块级别自适应地更新和重用缓存的中间动作特征，实现了对 Transformer-based Diffusion Policy 的无损加速。将特征缓存建模为常微分方程，通过预测-校准策略实现特征预测和误差修正，无需训练即可显著加速扩散变换器推理。,,,
2025.07,ICCV 2025,SkipCache,Accelerating Vision Diffusion Transformers with Skip Branches,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,https://arxiv.org/abs/2411.17616,https://github.com/OpenSparseLLMs/Skip-DiT,这项工作将长跳跃连接（LSCs）融入 DiT 中，以增强特征平滑性，从而实现一个稳定的静态缓存机制（Skip-Cache）。它在不显著损失质量的情况下，实现了约 1.5–2 倍的推理速度提升和约 4.4 倍的训练加速。,,,
2025.07,None,FoCa ,Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,https://arxiv.org/abs/2508.16211,https://github.com/Shenyi-Z/Cache4Diffusion/,提出FoCa方法，通过将扩散变换器的特征缓存建模为常微分方程（ODE），捕捉隐藏层表示随时间步的演化规律。FoCa采用“预测-校准”策略，先预测未来特征再进行校准，减少长步预测误差，实现稳定推理。该方法无需额外训练，即可在图像生成、视频生成和超分辨率等任务中显著加速推理，例如在FLUX和HunyuanVideo上分别实现约5.5倍和6.45倍加速，同时保持生成质量。FoCa在多种生成任务中表现优异，展示了高效、广泛适用的特征缓存优化方案。,,是,
2025.07,None,HiCache,HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,https://arxiv.org/abs/2508.16984,https://github.com/Shenyi-Z/Cache4Diffusion/,提出了一种名为 HiCache 的高效特征缓存方法，旨在加速扩散模型的推理过程，同时保持生成质量。通过分析扩散变换器中隐藏层特征的导数近似，发现其呈现多元高斯特性，启发性地采用 Hermite 多项式作为特征预测的理论最优基函数。,,是,
2025.07,None,WaveEx,WaveEx: Accelerating Flow Matching-based Speech Generation via Wavelet-guided Extrapolation,,音频生成,Shanghai Jiao Tong University,None,https://github.com/Shenyi-Z/Cache4Diffusion/,,,是,
2025.07,ICCV 2025,GOC,Accelerating Diffusion Transformer via Gradient-Optimized Cache,DiT,,University of Science and Technology of China,https://arxiv.org/pdf/2503.05156,https://github.com/qiujx0520/GOC_ICCV2025,提出GOC方法，通过梯度缓存传播补偿特征缓存引入的近似误差，并结合拐点感知优化，在特征变化关键节点对梯度进行对齐。该方法无需额外训练，即可在推理过程中显著提高扩散变换器效率，同时保持生成质量。,,,
2025.08,,TaoCache,TaoCache: Structure-Maintained Video Generation Acceleration,DiT,视频生成,Huawei,https://arxiv.org/pdf/2508.08978,None,采用固定点噪声预测与结构保持机制，通过余弦相似度和范数比校准噪声增量，无需训练即可高效加速视频生成，同时保持高分辨率结构信息。,,,
2025.08,ICCV 2025,OmniCache,OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models,DiT,视频生成,Zhipu AI,https://arxiv.org/abs/2508.16212,None,通过全局轨迹分析和曲率节点选择确定缓存重用策略，同时动态估计并滤除噪声，无需训练即可加速推理，并保持扩散变换器生成质量和稳定性。,,,
2025.08,,MixCache,MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration,DiT,视频生成,Sun Yat-sen University,https://arxiv.org/abs/2508.12691v1,None,提出混合缓存策略，结合上下文感知触发与自适应混合缓存决策，动态选择最佳缓存粒度，实现视频生成加速（如Wan 14B上提升1.94倍，HunyuanVideo上提升1.97倍），同时保持生成质量。,,,
2025.09,None,Z-Cache,Z-Cache: Accelerating Diffusion Transformers via  Self-Reflection,DiT,"图片生成, 视频生成",Shanghai Jiao Tong University,None,https://github.com/Shenyi-Z/Cache4Diffusion/,,,是,
,,,,,,,,,,,,
